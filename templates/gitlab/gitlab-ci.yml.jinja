# - DO NOT EDIT - FILE IS AUTOGENERATED

# NOTE: The docker:stable-dind service is not used here because --add-runtime=nvidia does not work from DIND and we need the
#       runtime to perform tests

# Important gitlab-runner considerations
#
# Docker buildx is used for multi-arch images builds. This feature is currently experimental and
# must be explicitly enabled on the docker daemon performing the image builds.
#
# To run multi-arch images on x86_64, qemu-user-static and systemd are required with the following configuration:
#
# $ cat /etc/binfmt.d/qemu-static.conf
# :qemu-aarch64:M::\x7fELF\x02\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\xb7:\xff\xff\xff\xff\xff\xff\xff\x00\xff\xff\xff\xff\xff\xff\xff\xff\xfe\xff\xff:/usr/bin/qemu-aarch64-static:CF
# :qemu-ppc64le:M::\x7fELF\x02\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x15\x00:\xff\xff\xff\xff\xff\xff\xff\x00\xff\xff\xff\xff\xff\xff\xff\xff\xfe\xff\xff\x00:/usr/bin/qemu-ppc64le-static:CF
#
# < reboot or start systemd-binfmt.service >
#
# Check with:
#
# $ systemctl status systemd-binfmt.service
# < service should be started >
#
# $ docker run -it nvidia/cuda-ppc64le:11.0-base-ubuntu18.04-rc
# < container should run>

variables:
  # Need a value of two here for checking the manifest in the last commit
  GIT_DEPTH: "2"
  # If set to "true" this will force the prepare stage to run to build the builder images that contains docker buildx and python dependencies
  REBUILD_BUILDER: "false"
  {# NO_PUSH: "true" #}
  {# NO_SCAN: "true" # Only use in development #}
  {# NO_TEST: "true" # Only use in development #}

before_script:
  - source util.sh
  - if [[ ! -z ${RELEASE_LABEL} ]]; then
        export CUDA_VERSION=${RELEASE_LABEL};
    fi
  - 'echo "TRIGGER: ${KITMAKER}"'
  - 'echo "KITMAKER: ${KITMAKER}"'
  - 'echo "IMAGE_NAME: $IMAGE_NAME"'
  - 'echo "OS: $OS"'
  - 'echo "OS_VERSION: $OS_VERSION"'
  - 'echo "OS_NAME: $OS_NAME"'
  - 'echo "ARCH: $ARCH"'
  - 'echo "CUDA_VERSION: $CUDA_VERSION"'
  - 'echo "RELEASE_LABEL: $RELEASE_LABEL"' # used by kitmaker and cuda versions >= 11.2
  - 'echo "CUDNN_VERSION: $CUDNN_VERSION"'
  - 'echo "IMAGE_TAG_SUFFIX: $IMAGE_TAG_SUFFIX"'
  - 'echo "NO_OS_SUFFIX: $NO_OS_SUFFIX"'
  # Gitlab is used to stage images
  - retry 5 20 docker login -u "gitlab-ci-token" -p $CI_JOB_TOKEN gitlab-master.nvidia.com:5005
  # Login to NGC to pull images
  - retry 5 20 docker login -u '$oauthtoken' -p $NVCR_TOKEN nvcr.io;

.security_scan:
  image: gitlab-master.nvidia.com:5005/pstooling/pulse-group/pulse-container-scanner/pulse-cli:latest
  variables:
    PSS_SSA_ID: "x9thwm-cootr2q1jdv5p7b8iw4fs4ob3x6nqqsoznyk"
    PSS_SSA_SCOPE: "nspect.verify%20scan.anchore"
    SSA_ISSUER_URL: "https://${PSS_SSA_ID}.ssa.nvidia.com/token?grant_type=client_credentials&scope=${PSS_SSA_SCOPE}"
    IMAGE_ARCHIVE: "cuda-${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}.tar"

stages:
  - prepare
  - trigger
  - cuda
  - cudnn
  - test
  - scan
  - deploy
  - cleanup_success
  - cleanup_failure

# builds the gitlab-cuda-builder image
prepare:
  image: docker:stable
  stage: prepare
  variables:
{% set gitlab_builder_image = "gitlab-master.nvidia.com:5005/cuda-installer/cuda/gitlab-cuda-builder" %}
    IMAGE_NAME: "{{ gitlab_builder_image }}"
  script:
    - echo ">>>> PYPROJECT.TOML >>>>"
    - cat pyproject.toml
    - docker build -t {{ gitlab_builder_image }} --cache-from {{ gitlab_builder_image }} .
    - docker push {{ gitlab_builder_image }}
  tags:
    - docker
  rules:
      - if: '$REBUILD_BUILDER == "true"'

.tags_template: &tags_definition
  tags:
    - docker

.tags_template_multiarch: &tags_definition_multiarch
  tags:
    - docker-multi-arch

# Only used for CUDA 8.0, will be removed once CUDA 8 support is dropped
.cuda_template_depricated: &cuda_definition_deprecated
  image: {{ gitlab_builder_image }}
  stage: cuda
  script:
    - if [[ "${NO_OS_SUFFIX}" == "true" ]]; then
        export TAG_RUNTIME=" -t ${IMAGE_NAME}:${CUDA_VERSION}-runtime ";
        export TAG_DEVEL=" -t ${IMAGE_NAME}:${CUDA_VERSION}-devel ";
        export CUDA_TAGS="${CUDA_VERSION}-runtime ${CUDA_VERSION}-devel ";
      fi
    - export DIST_BASE_PATH="${DIST_BASE_PATH}/${OS//.}";
    - export PLATFORM_ARG=`printf '%s ' '--platform'; for var in $(echo $ARCHES | sed "s/,/ /g"); do printf 'linux/%s,' "$var"; done | sed 's/,*$//g'`
    - 'echo "PLATFORM_ARG: ${PLATFORM_ARG}"'
    # COPY the NGC DEEP LEARNING CONTAINER LICENSE
    - run_cmd cp -v ./NGC-DL-CONTAINER-LICENSE "${DIST_BASE_PATH}/runtime"
    - run_cmd retry 3 10 docker buildx create --use ${PLATFORM_ARG} --driver-opt image=moby/buildkit:{{ cuda.moby_buildkit_version }} --name cuda
    - run_cmd retry 3 10 docker buildx build --pull --push ${PLATFORM_ARG} -t "${IMAGE_NAME}:${CUDA_VERSION}-runtime-${OS}" ${TAG_RUNTIME} "${DIST_BASE_PATH}/runtime"
                          --build-arg BUILDKIT_INLINE_CACHE=1 --progress plain
                          --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-runtime-${OS}
    - run_cmd retry 3 10 docker buildx build --pull --push ${PLATFORM_ARG} -t "${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}" ${TAG_DEVEL} "${DIST_BASE_PATH}/devel"
                          --build-arg "IMAGE_NAME=${IMAGE_NAME}"
                          --build-arg BUILDKIT_INLINE_CACHE=1 --progress plain
                          --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}
    - 'echo "CUDA_TAGS=\"${CUDA_TAGS} ${CUDA_VERSION}-runtime-${OS} ${CUDA_VERSION}-devel-${OS}\"" >> tags.env'
    - cat tags.env
  artifacts:
    reports:
      dotenv: tags.env

.cuda_base_template: &cuda_base_definition
  image: {{ gitlab_builder_image }}
  stage: cuda
  script:
    - |
      # How gitlab handles errors: https://gitlab.com/gitlab-org/gitlab-runner/-/issues/25394
      # prevent exit codes from killing the pipeline completely, errors must now be handled manually
      set +e
      if [[ "${NO_OS_SUFFIX}" == "true" ]]; then
        export TAG_BASE=" -t ${IMAGE_NAME}:${CUDA_VERSION}-base${IMAGE_TAG_SUFFIX} "
        export TAG_RUNTIME=" -t ${IMAGE_NAME}:${CUDA_VERSION}-runtime${IMAGE_TAG_SUFFIX} "
        export TAG_DEVEL=" -t ${IMAGE_NAME}:${CUDA_VERSION}-devel${IMAGE_TAG_SUFFIX} "
        export CUDA_TAGS="${CUDA_VERSION}-base${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-runtime${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-devel${IMAGE_TAG_SUFFIX} "
      fi
      export DIST_BASE_PATH="${DIST_BASE_PATH}/${OS//.}"
      if [ ! -z $KITMAKER ] && [ ! -z $TRIGGER ]; then
          export DIST_BASE_PATH="kitpick/${OS//.}"
          rm -rf kitpick || true
          run_cmd mkdir -p kitpick/${OS//.}/{devel,runtime,base}

          NEW_CANDIDATE_URL="${CANDIDATE_URL}/${OS/./}"
          # Ensure the url ends with a slash! This prevents wget from getting everything (and bringing apache to its knees)
          [[ "${NEW_CANDIDATE_URL}" != */ ]] && NEW_CANDIDATE_URL="${NEW_CANDIDATE_URL}/"
          echo "NEW_CANDIDATE_URL: ${NEW_CANDIDATE_URL}"

          if ! retry 3 10 run_cmd wget --no-verbose --recursive --directory-prefix="kitpick_dl" --reject="index.html*" --no-parent ${NEW_CANDIDATE_URL}; then
              echo "Could not download files from '${CANDIDATE_URL}'!!"
              # kitmaker_webhook_failed
              exit 1
          fi

          run_cmd cp -R kitpick_dl/${NEW_CANDIDATE_URL#"http://"}/{base,devel,runtime} ${DIST_BASE_PATH}/
          echo "Contents of ${DIST_BASE_PATH}/"
          find ${DIST_BASE_PATH}/
      fi
      export PLATFORM_ARG=`printf '%s ' '--platform'; for var in $(echo $ARCHES | sed "s/,/ /g"); do printf 'linux/%s,' "$var"; done | sed 's/,*$//g'`
      echo "PLATFORM_ARG: ${PLATFORM_ARG}"
      echo "DIST_BASE_PATH: ${DIST_BASE_PATH}"
      echo "TAG_BASE: ${TAG_BASE}"
      echo "TAG_RUNTIME: ${TAG_RUNTIME}"
      echo "TAG_DEVEL: ${TAG_DEVEL}"
      # COPY the NGC DEEP LEARNING CONTAINER LICENSE
      run_cmd cp -v ./NGC-DL-CONTAINER-LICENSE "${DIST_BASE_PATH}/base"
      # COPY the entrypoint scripts
      run_cmd cp -R entrypoint.d nvidia_entrypoint.sh ${DIST_BASE_PATH}/runtime/
      # --driver-opt flag is needed to workaround https://github.com/docker/buildx/issues/386
      if ! run_cmd retry 3 10 docker buildx create --use ${PLATFORM_ARG} --driver-opt image=moby/buildkit:{{ cuda.moby_buildkit_version }} --name cuda; then
          echo "Failed creating buildx instance!"
          # kitmaker_webhook_failed
          exit 1
      fi
      echo "####################################################################"
      echo "BASE"
      echo "####################################################################"
      if ! run_cmd retry 3 10 docker buildx build --pull --push ${PLATFORM_ARG} -t "${IMAGE_NAME}:${CUDA_VERSION}-base-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_BASE} "${DIST_BASE_PATH}/base" \
                            --build-arg BUILDKIT_INLINE_CACHE=1 --progress plain \
                            --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-base-${OS}${IMAGE_TAG_SUFFIX}; then
          echo "Failed building the base image!"
          # kitmaker_webhook_failed
          exit 1
      fi
      echo "####################################################################"
      echo "RUNTIME"
      echo "####################################################################"
      if ! run_cmd retry 3 10 docker buildx build --pull --push ${PLATFORM_ARG} -t "${IMAGE_NAME}:${CUDA_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_RUNTIME} \
                            --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/runtime" \
                            --build-arg BUILDKIT_INLINE_CACHE=1 --progress plain \
                            --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}; then
          echo "Failed building the runtime image!"
          # kitmaker_webhook_failed
          exit 1
      fi
      echo "####################################################################"
      echo "DEVEL"
      echo "####################################################################"
      if ! run_cmd retry 3 10 docker buildx build --pull --push ${PLATFORM_ARG} -t "${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_DEVEL} \
                            --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/devel" \
                            --build-arg BUILDKIT_INLINE_CACHE=1 --progress plain \
                            --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}; then
          echo "Failed building the devel image!"
          # kitmaker_webhook_failed
          exit 1
      fi
      echo "CUDA_TAGS=\"${CUDA_TAGS} ${CUDA_VERSION}-base-${OS}${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}\"" >> tags.env
      cat tags.env
  artifacts:
    reports:
      dotenv: tags.env

.cudnn_template: &cudnn_definition
  image: {{ gitlab_builder_image }}
  stage: cudnn
  script:
    - |
      # no set +e necessary here since we don't use this yet for kitmaker builds
      DIST_BASE_PATH="${DIST_BASE_PATH}/${OS/./}";
      if [ ! -z $KITMAKER ] && [ ! -z $TRIGGER ]; then
          export DIST_BASE_PATH="kitpick/${OS/./}"
          rm -rf kitpick || true
          run_cmd mkdir -p kitpick/${OS/./}/{devel,runtime,base}
          NEW_CANDIDATE_URL="${CANDIDATE_URL}/${OS/./}"
          # Ensure the url ends with a slash! This prevents wget from getting everything (and bringing apache to its knees)
          [[ "${CANDIDATE_URL}" != */ ]] && NEW_CANDIDATE_URL="${NEW_CANDIDATE_URL}/"
          echo "NEW_CANDIDATE_URL: ${NEW_CANDIDATE_URL}"
          if ! retry 3 10 run_cmd wget --no-verbose --recursive --directory-prefix="kitpick_dl" --reject="index.html*" --no-parent ${NEW_CANDIDATE_URL}; then
              echo "Could not download files from '${CANDIDATE_URL}'!!"
              # kitmaker_webhook_failed
              exit 1
          fi
          run_cmd cp -R kitpick_dl/${NEW_CANDIDATE_URL#"http://"}/{base,devel,runtime} ${DIST_BASE_PATH}/
          echo "Contents of ${DIST_BASE_PATH}/"
          find kitpick/
          export CUDNN_VERSION="cudnn$(grep -r "CUDNN_VERSION" kitpick/ | head -n 1 | cut -f 3 -d" " | tr -d '[:space:]' | cut -c 1)"
      fi
      echo "DIST_BASE_PATH: ${DIST_BASE_PATH}"
      echo "CUDNN_VERSION: ${CUDNN_VERSION}"
      if [[ "${NO_OS_SUFFIX}" == "true" ]]; then
          TAG_CUDNN_RUNTIME=" -t ${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-runtime${IMAGE_TAG_SUFFIX} ";
          TAG_CUDNN_DEVEL=" -t ${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-devel${IMAGE_TAG_SUFFIX} ";
          export CUDA_EXTRA_TAGS="${CUDA_EXTRA_TAGS} ${CUDA_VERSION}-${CUDNN_VERSION}-runtime${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-${CUDNN_VERSION}-devel${IMAGE_TAG_SUFFIX}";
      fi
      export PLATFORM_ARG=`printf '%s ' '--platform'; for var in $(echo $ARCHES | sed "s/,/ /g"); do printf 'linux/%s,' "$var"; done | sed 's/,*$//g'`
      echo "PLATFORM_ARG: ${PLATFORM_ARG}"
      if ! run_cmd retry 3 10 docker buildx create --use ${PLATFORM_ARG} --driver-opt image=moby/buildkit:{{ cuda.moby_buildkit_version }} --name cuda; then
          echo "Failed creating buildx instance!"
          # kitmaker_webhook_failed
          exit 1
      fi
      echo "####################################################################"
      echo "CUDNN RUNTIME"
      echo "####################################################################"
      if ! run_cmd retry 3 10 docker buildx build --pull --push ${PLATFORM_ARG} -t "${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_CUDNN_RUNTIME} \
                  --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/runtime/${CUDNN_VERSION}" \
                  --build-arg BUILDKIT_INLINE_CACHE=1 --progress plain \
                  --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}; then
          echo "Failed building the cudnn runtime image!"
          # kitmaker_webhook_failed
          exit 1
      fi
      echo "####################################################################"
      echo "CUDNN DEVEL"
      echo "####################################################################"
      if ! run_cmd retry 3 10 docker buildx build --pull --push ${PLATFORM_ARG} -t "${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}" ${TAG_CUDNN_DEVEL} \
                  --build-arg "IMAGE_NAME=${IMAGE_NAME}" "${DIST_BASE_PATH}/devel/${CUDNN_VERSION}" \
                  --build-arg BUILDKIT_INLINE_CACHE=1 --progress plain \
                  --cache-from=type=registry,ref=${IMAGE_NAME}:${CUDA_VERSION}-${CUDNN_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}; then
          echo "Failed building the cudnn devel image!"
          # kitmaker_webhook_failed
          exit 1
      fi
      # this is not a great way to pass information between stages, but it is currently the only way
      echo "CUDA_TAGS=${CUDA_TAGS}" > cudnn_tags.env
      echo "CUDA_${CUDNN_VERSION}_TAGS=\"${CUDA_VERSION}-${CUDNN_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX} ${CUDA_VERSION}-${CUDNN_VERSION}-runtime-${OS}${IMAGE_TAG_SUFFIX}\"" >> cudnn_tags.env
      echo "CUDA_EXTRA_${CUDNN_VERSION}_TAGS=\"${CUDA_EXTRA_TAGS}\"" >> cudnn_tags.env
      cat cudnn_tags.env
  artifacts:
    reports:
      dotenv: cudnn_tags.env

.test_template: &test_definition
  image: {{ gitlab_builder_image }}
  stage: test
  tags:
    - cuda-test
  script:
    - |
      set +e
      image=${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}
      rmicmd="docker rmi -f ${image}"
      # prunecmd="docker image prune --filter=dangling=true -f"
      if ! bash -e ./test/scripts/bats_install.sh; then
          echo "ERROR: Could not install bats-core!"
          exit 1
      fi
      if ! bash -e ./test/scripts/run_tests.sh; then
          echo "ERROR: Tests failed!"
          ${rmicmd}
          exit 1
      fi
      ${rmicmd}

.scan_template: &scan_definition
  extends:
    - .security_scan
  stage: scan
  <<: *tags_definition
  script:
    - |
      docker pull ${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}
      # saving docker image to archive to run the security scan against it in later steps
      docker save ${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX} > ${IMAGE_ARCHIVE}
      docker rmi -f ${IMAGE_NAME}:${CUDA_VERSION}-devel-${OS}${IMAGE_TAG_SUFFIX}

      echo "Connection request with Pulse Security Service using SSA..."

      AuthHeader=$(echo -n $SSA_CLIENT_ID:$SSA_CLIENT_SECRET | base64 | tr -d '\n')
      export SSA_TOKEN=$(curl --request POST --header "Authorization: Basic $AuthHeader" --header "Content-Type: application/x-www-form-urlencoded" ${SSA_ISSUER_URL} | jq ".access_token" |  tr -d '"')

      if [ -z "$SSA_TOKEN" ]; then
          exit 1;
      else
          echo "SSA Token has been set successfully!";
      fi

      pulse-cli -n $NSPECT_ID --ssa $SSA_TOKEN scan -i $IMAGE_ARCHIVE -o
      retval=$?

      echo "Pulse Scan return value: ${retval}"

      rm $IMAGE_ARCHIVE
      if [[ $retval -ne 0 ]]; then
          exit 1;
      fi
  artifacts:
    when: always
    expire_in: 1 week
    paths:
      - pulse-cli.log
      - licenses.json
      - sbom.json
      - vulns.json

.deploy_template: &deploy_definition
  image: {{ gitlab_builder_image }}
  stage: deploy
  <<: *tags_definition
  script:
    - |
      set -x
      # How gitlab handles errors: https://gitlab.com/gitlab-org/gitlab-runner/-/issues/25394
      # prevent exit codes from killing the pipeline completely, errors must now be handled manually
      set +e
      if [[ -z ${TAG_MANIFEST_PATH} ]]; then export TAG_MANIFEST_PATH=TAG_MANIFEST; fi
      for tag in $(env | grep 'CUDA.*_TAGS' | cut -f2 -d=); do
          {# [[ $tag == "\"\"" ]] && continue; #}
          echo ${tag//\"} >> ${TAG_MANIFEST_PATH}
      done
      echo ">>> BEGIN ${TAG_MANIFEST_PATH} <<<"
      cat ${TAG_MANIFEST_PATH}
      echo ">>> END ${TAG_MANIFEST_PATH} <<<"
      if [ ! -z $KITMAKER ] && [ ! -z $TRIGGER ]; then
          manifest_file="manifest.yml"
          if ! retry 3 10 wget --no-verbose --directory-prefix="kitpick" --reject="index.html*" --no-parent ${CANDIDATE_URL}/${manifest_file}; then
              echo "Could not download files from '${CANDIDATE_URL}'!!"
              # kitmaker_webhook_failed
              exit 1
          fi
          export MANIFEST="kitpick/${manifest_file}";
      fi
      run_cmd skopeo --version
      if [[ "${CI_DEFAULT_BRANCH}" != "${CI_COMMIT_BRANCH}" ]]; then
          echo -e "\e[31m\e[1m>>>> WARNING <<<<\e[0m"
          echo -e "\e[31m\e[1m>>>> Push uses protected variables and the current branch (${CI_COMMIT_BRANCH}) IS NOT the default branch! (${CI_DEFAULT_BRANCH}) Failures may occur! <<<<\e[0m"
          echo -e "\e[31m\e[1m>>>> WARNING <<<<\e[0m"
      fi
      # TODO: switch to zippapp or pipx, this is just silly
      export PATH=$PATH:/root/.local/bin
      source /root/cuda_manager_env/bin/activate
      if ! run_cmd retry 3 10 poetry run ./manager.py ${MANIFEST:+`echo "--manifest ${MANIFEST}"`} push \
              --tag-manifest ${TAG_MANIFEST_PATH} \
              --image-name "${IMAGE_NAME}" \
              --os-name "${OS_NAME}" \
              ${OS_VERSION:+`echo "--os-version ${OS_VERSION}"`} \
              --cuda-version "${CUDA_VERSION}" \
              ${PIPELINE_NAME:+`echo "--pipeline-name ${PIPELINE_NAME}"`} \
              ${IMAGE_TAG_SUFFIX:+`echo "--tag-suffix ${IMAGE_TAG_SUFFIX}"`} ${NO_PUSH:+"-n"}; then
          echo "Error pushing images!"
          # kitmaker_webhook_failed
          exit 1
      fi
      cat $MANIFEST | grep ".*urm.nvidia.com\/sw-gpu-cuda-installer-docker-local\/" | cut -f2 -d: | head -n 1 | xargs >> ${TAG_MANIFEST_PATH}
      run_cmd cat ${TAG_MANIFEST_PATH}
      set +x
      # kitmaker_cleanup_webhook_success
  artifacts:
    paths:
      - ${TAG_MANIFEST_PATH}

trigger:
  image: {{ gitlab_builder_image }}
  stage: trigger
  <<: *tags_definition
  variables:
    MANIFEST: "{{ cuda.manifest_path }}"
  script:
    - echo CI_COMMIT_MESSAGE:$CI_COMMIT_MESSAGE
    - export ESC_TO=`echo $TRIGGER_OVERRIDE | sed 's/\ //g'`
    - export CMD="poetry run ./manager.py ${MANIFEST:+`echo --manifest ${MANIFEST}`} trigger ${TRIGGER_OVERRIDE:+`echo --trigger-override ${ESC_TO}`}"
    - 'echo "COMMAND: $CMD"'
    - source /root/cuda_manager_env/bin/activate
    - $CMD
  rules:
    - if: '$TRIGGER_OVERRIDE'

{% for data in cuda.supported_distro_matrix.list %}
    {% set flavor = "" %}
    {% set kitmaker_distro = data.full_name() %}
    {% set kitmaker_distro_clean = data.shipit_distro_name() %}
    {% if cuda.supported_distro_matrix.by_distro(kitmaker_distro_clean) %}
        {% set arches = cuda.supported_distro_matrix.by_distro(kitmaker_distro_clean).container_arches_csv() %}
    {% endif %}
    {% if data.flavor %}
        {% set flavor = data.flavor + "_" %}
        {% set kitmaker_distro_clean = data.distro.replace("-", "_") + data.version.replace(".", "_") + "_" + data.flavor %}
    {% endif %}
# Need to pass these from the trigger
.kitmaker_{{ kitmaker_distro_clean }}_variables: &kitmaker_{{ kitmaker_distro_clean }}_variables
    {% if data.image_name %}
  IMAGE_NAME: "{{ data.image_name }}"
    {% else %}
  IMAGE_NAME: "gitlab-master.nvidia.com:5005/cuda-installer/cuda/release-candidate/cuda"
    {% endif %}
  OS: "{{ data.distro }}{{ data.version }}"
  OS_NAME: "{{ data.distro }}"
  OS_VERSION: "{{ data.version }}"
  ARCHES: "{{ arches }}"
    {% if data.flavor %}
  FLAVOR: "{{ data.flavor }}"
    {% endif %}
  # CI_DEBUG_TRACE: "true"

.kitmaker_{{ kitmaker_distro_clean }}_only: &kitmaker_{{ kitmaker_distro_clean }}_only
  image: {{ gitlab_builder_image }}
  variables:
    <<: *kitmaker_{{ kitmaker_distro_clean }}_variables
    TAG_MANIFEST_PATH: "tag_manifest_{{ kitmaker_distro_clean }}"
  rules:
    {% if "l4t" in kitmaker_distro_clean %}
    - if: '$TRIGGER == "true" && $KITMAKER == "true" && $FLAVOR == "jetson"'
    {% else %}
    - if: '$TRIGGER == "true" && $KITMAKER == "true" && $FLAVOR == "default"'
    - if: ${{kitmaker_distro_clean | upper }} == "true"
    {% endif %}

kitmaker_{{ kitmaker_distro }}_{{ flavor }}base:
  <<: *cuda_base_definition
  <<: *kitmaker_{{ kitmaker_distro_clean }}_only
  # Need to build container images on a multi-arch capable machine
  <<: *tags_definition_multiarch
{% if data.flavor == "jetson" %}

# L4T only, will be removed eventually!
kitmaker_{{ kitmaker_distro }}_{{ flavor }}cudnn:
  <<: *cudnn_definition
  <<: *kitmaker_{{ kitmaker_distro_clean }}_only
  # Need to build container images on a multi-arch capable machine
  <<: *tags_definition_multiarch
  artifacts:
    reports:
      dotenv: cudnn_tags.env
{% endif %}

kitmaker_{{ kitmaker_distro }}_{{ flavor }}deploy:
  <<: *deploy_definition
  <<: *kitmaker_{{ kitmaker_distro_clean }}_only
  <<: *tags_definition
  needs:
    - job: kitmaker_{{ kitmaker_distro }}_{{ flavor }}base
      artifacts: true
    {% if data.flavor == "jetson" %}
    - job: kitmaker_{{ kitmaker_distro }}_{{ flavor }}cudnn
      optional: true
    {% endif %}
  artifacts:
    paths:
      - tag_manifest_{{ kitmaker_distro_clean }}

{% if data.flavor == "jetson" %}
kitmaker_{{ kitmaker_distro }}_{{ flavor }}success:
  stage: cleanup_success
  image: {{ gitlab_builder_image }}
  <<: *tags_definition
  rules:
    - if: '$TRIGGER == "true" && $KITMAKER == "true" && $FLAVOR == "jetson"'
      when: on_success
  script:
    - |
      kitmaker_cleanup_webhook_success

kitmaker_{{ kitmaker_distro }}_{{ flavor }}failure:
  stage: cleanup_failure
  image: {{ gitlab_builder_image }}
  <<: *tags_definition
  rules:
    - if: '$TRIGGER == "true" && $KITMAKER == "true" && $FLAVOR == "jetson"'
      when: on_failure
  script:
    - |
      kitmaker_webhook_failed
{% endif %}

{% endfor %}

kitmaker_success:
  stage: cleanup_success
  image: {{ gitlab_builder_image }}
  <<: *tags_definition
  rules:
    - if: '$TRIGGER == "true" && $KITMAKER == "true" && $FLAVOR == "default"'
      when: on_success
  needs:
{% set stahp = { 'flag': True } %}
{% for data in kitmaker_distro_data %}
    {% if data.flavor %}
        {% continue %}
    {% endif %}
  {% set kitmaker_distro = data.distro + data.version %}
  {% set kitmaker_distro_clean = data.distro + data.version.replace(".", "_") %}
    - job: kitmaker_{{ kitmaker_distro }}_{{ flavor }}deploy
    {% if stahp.flag %}
      artifacts: true
      {% do stahp.update({'flag':False}) %}
    {% endif %}
{% endfor %}
  script:
    - |
      kitmaker_cleanup_webhook_success

kitmaker_failure:
  stage: cleanup_failure
  image: {{ gitlab_builder_image }}
  <<: *tags_definition
  rules:
    - if: '$TRIGGER == "true" && $KITMAKER == "true" && $FLAVOR == "default"'
      when: on_failure
  script:
    - |
      kitmaker_webhook_failed

{% for cuda_version in cuda if cuda_version is iterable and "." in cuda_version %}
{% for pipeline in cuda[cuda_version] %}
{% if "distros" in cuda[cuda_version][pipeline] %}
{% for distro in cuda[cuda_version][pipeline]["distros"] %}

{% set cuda_version_yaml_safe = cuda[cuda_version][pipeline]["cuda_version_yaml_safe"] %}
{% set distro_yaml_safe = cuda[cuda_version][pipeline]["distros"][distro]["yaml_safe"] -%}
{% set image_tag_suffix = cuda[cuda_version][pipeline]["distros"][distro]["image_tag_suffix"] -%}
{% set pipeline_name_us = "_" + pipeline if pipeline != "default" else "" -%}
{% set pipeline_name_hyp = "-" + pipeline if pipeline != "default" else "" -%}

.{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_variables: &{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_variables
  DIST_BASE_PATH: "{{ cuda[cuda_version][pipeline]["dist_base_path"] }}"
  IMAGE_NAME: "{{ cuda[cuda_version][pipeline]["image_name"] }}"
  MANIFEST: "{{ cuda.manifest_path }}"
  ARCHES: "{{ cuda[cuda_version][pipeline]['distros'][distro]['arches'] | join(', ') }}"
  {% if image_tag_suffix %}
  IMAGE_TAG_SUFFIX: {{ image_tag_suffix }}
  {% endif %}
  {% if cuda[cuda_version][pipeline]["distros"][distro]["no_os_suffix"] %}
  NO_OS_SUFFIX: "true"
  {% endif %}
  OS: "{{ distro }}"
  OS_NAME: "{{ cuda[cuda_version][pipeline]["distros"][distro]["name"] }}"
  OS_VERSION: "{{ cuda[cuda_version][pipeline]["distros"][distro]["version"] }}"
  CUDA_VERSION: "{{ cuda_version }}"
  {% if pipeline != "default" %}
  PIPELINE_NAME: "{{ pipeline }}"
  {% endif %}

.{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_only: &{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_only
  <<: *tags_definition_multiarch
  variables:
    <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_variables
  rules:
    - if: '${{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }} == "true"'
    - if: '$all == "true"'

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}:
{% if cuda_version == "8.0" %}
  <<: *cuda_definition_deprecated
{% else %}
  <<: *cuda_base_definition
{% endif %}
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_only

{% set cudnn_stages = [] %}
{% if cuda[cuda_version][pipeline]["distros"][distro]["cudnn"] %}
{# FIXME; build list of cudnn versions for each arch instead of just using the versions for x86_64 #}
{% for cudnn in cuda[cuda_version][pipeline]["distros"][distro]["cudnn"]["x86_64"] -%}

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-{{ cudnn }}:
  <<: *cudnn_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_only
  # Variables overwritten here. Don't move this section.
  variables:
    <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_variables
    CUDNN_VERSION: "{{ cudnn }}"
    ARCHES: "{{ cuda[cuda_version][pipeline]['distros'][distro]['cudnn'] | join(', ') }}"
  needs:
    - job: {{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}
      artifacts: true

{% if pipeline and pipeline != "default" %}
  {% do cudnn_stages.append(distro + "-v" + cuda_version + "-" + pipeline + "-" + cudnn) %}
{% else %}
  {% do cudnn_stages.append(distro + "-v" + cuda_version + "-" + cudnn) %}
{% endif %}
{% endfor %}
{% endif -%}

{% set ns = namespace(needs = ["{}-v{}{}".format(distro, cuda_version, pipeline_name_hyp)]) %}
{% if cudnn_stages %}
    {# {% set needs = [] %} #}
    {% for cudnn_stage in cudnn_stages %}
        {% do ns.needs.append(cudnn_stage) %}
    {% endfor %}
{% endif -%}

{% for arch in cuda[cuda_version][pipeline]["distros"][distro]["arches"] -%}
{{distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-test-{{ arch }}:
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_only
  <<: *test_definition
  variables:
    <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_variables
    ARCH: "{{ arch }}"
  rules:
    - if: '$NO_TEST == "true"'
      when: never
    - if: '${{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }} == "true"'
    - if: '$all == "true"'
  needs:
  {# This silly workaround is needed because variables lose scope in a for loop #}
  {% set stahp = { 'flag': True } %}
  {% for job in ns.needs %}
    - job: {{ job }}
    {% if stahp.flag %}
      artifacts: true
      {% do stahp.update({'flag':False}) %}
    {% else %}
      optional: true
    {% endif %}
  {% endfor %}

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-scan-{{ arch }}:
  <<: *scan_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_only
  needs:
    - job: {{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}
    - job: {{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-test-{{ arch }}
      optional: true
  variables:
    <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_variables
    ARCH: "{{ arch }}"
  # Override rules from only definition
  rules:
    - if: '$NO_SCAN == "true" || $NO_TEST == "true"'
      when: never
    - if: '${{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }} == "true"'
    - if: '$all == "true"'

{% endfor -%}

{% for arch in cuda[cuda_version][pipeline]["distros"][distro]["arches"] -%}
{% set ns.needs = ns.needs + [
    "{}-v{}{}-test-{}".format(distro, cuda_version, pipeline_name_hyp, arch),
    "{}-v{}{}-scan-{}".format(distro, cuda_version, pipeline_name_hyp, arch)] %}
{% endfor -%}

{{ distro }}-v{{ cuda_version }}{{ pipeline_name_hyp }}-deploy:
  <<: *deploy_definition
  <<: *{{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }}_only
  # Override rules from only definition
  rules:
    - if: '($NO_TEST == "true" || $NO_SCAN == "true") && $NO_PUSH == "false"'
      when: never
    - if: '${{ distro_yaml_safe }}_{{ cuda_version_yaml_safe }}{{ pipeline_name_us }} == "true"'
    - if: '$all == "true"'
  needs:
  {# This silly workaround is needed because variables lose scope in a for loop #}
  {% set stahp = { 'flag': True } %}
  {% for job in ns.needs %}
    - job: {{ job }}
    {% if stahp.flag %}
      artifacts: true
      {% do stahp.update({'flag':False}) %}
    {% else %}
      optional: true
    {% endif %}
  {% endfor %}

{% endfor -%}
{% endif %}
{% endfor %}
{% endfor %}
